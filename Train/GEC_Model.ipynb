{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc1oulfe1WSJ",
        "outputId": "74b30a35-8759-413b-87d9-d4447217318e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
            "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to /tmp/pip-req-build-u_tlynj9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Gramformer.git /tmp/pip-req-build-u_tlynj9\n",
            "  Resolved https://github.com/PrithivirajDamodaran/Gramformer.git to commit 23425cd2e98a919384cab6156af8adf1c9d0639a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (4.38.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (0.1.99)\n",
            "Collecting python-Levenshtein (from gramformer==1.0)\n",
            "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting fuzzywuzzy (from gramformer==1.0)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (0.15.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gramformer==1.0) (2023.6.0)\n",
            "Collecting errant (from gramformer==1.0)\n",
            "  Downloading errant-3.0.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.3/499.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from errant->gramformer==1.0) (3.7.4)\n",
            "Collecting rapidfuzz>=3.4.0 (from errant->gramformer==1.0)\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Levenshtein==0.25.1 (from python-Levenshtein->gramformer==1.0)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->gramformer==1.0) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->gramformer==1.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->gramformer==1.0) (4.11.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->gramformer==1.0) (2024.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.16.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.1.5)\n",
            "Building wheels for collected packages: gramformer\n",
            "  Building wheel for gramformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gramformer: filename=gramformer-1.0-py3-none-any.whl size=4463 sha256=a7fedc2c121d866179079c7328ca3adf762a2f271e6c082673e7d14697756875\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p8l6gpri/wheels/76/44/15/e79b5dc4f5c897b2054e6a8e357f6de157b9554f072a2e56ea\n",
            "Successfully built gramformer\n",
            "Installing collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein, errant, gramformer\n",
            "Successfully installed Levenshtein-0.25.1 errant-3.0.0 fuzzywuzzy-0.18.0 gramformer-1.0 python-Levenshtein-0.25.1 rapidfuzz-3.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gramformer:\n",
        "\n",
        "  def __init__(self, models=1, use_gpu=False):\n",
        "    from transformers import AutoTokenizer\n",
        "    from transformers import AutoModelForSeq2SeqLM\n",
        "    #from lm_scorer.models.auto import AutoLMScorer as LMScorer\n",
        "    import errant\n",
        "    self.annotator = errant.load('en')\n",
        "\n",
        "    if use_gpu:\n",
        "        device= \"cuda:0\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    batch_size = 1\n",
        "    #self.scorer = LMScorer.from_pretrained(\"gpt2\", device=device, batch_size=batch_size)\n",
        "    self.device    = device\n",
        "    correction_model_tag = \"deepak/grammar_error_correcter_v1\"\n",
        "    self.model_loaded = False\n",
        "\n",
        "    if models == 1:\n",
        "        self.correction_tokenizer = AutoTokenizer.from_pretrained(correction_model_tag, use_auth_token=False)\n",
        "        self.correction_model     = AutoModelForSeq2SeqLM.from_pretrained(correction_model_tag, use_auth_token=False)\n",
        "        self.correction_model     = self.correction_model.to(device)\n",
        "        self.model_loaded = True\n",
        "        print(\"[Gramformer] Grammar error correct/highlight model loaded..\")\n",
        "    elif models == 2:\n",
        "        # TODO\n",
        "        print(\"TO BE IMPLEMENTED!!!\")\n",
        "\n",
        "  def correct(self, input_sentence, max_candidates=1):\n",
        "      if self.model_loaded:\n",
        "        correction_prefix = \"gec: \"\n",
        "        input_sentence = correction_prefix + input_sentence\n",
        "        input_ids = self.correction_tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "        input_ids = input_ids.to(self.device)\n",
        "\n",
        "        preds = self.correction_model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            max_length=128,\n",
        "           #top_k=50,\n",
        "           #top_p=0.95,\n",
        "            num_beams=7,\n",
        "            early_stopping=True,\n",
        "            num_return_sequences=max_candidates)\n",
        "\n",
        "        corrected = set()\n",
        "        for pred in preds:\n",
        "          corrected.add(self.correction_tokenizer.decode(pred, skip_special_tokens=True).strip())\n",
        "\n",
        "        #corrected = list(corrected)\n",
        "        #scores = self.scorer.sentence_score(corrected, log=True)\n",
        "        #ranked_corrected = [(c,s) for c, s in zip(corrected, scores)]\n",
        "        #ranked_corrected.sort(key = lambda x:x[1], reverse=True)\n",
        "        return corrected\n",
        "      else:\n",
        "        print(\"Model is not loaded\")\n",
        "        return None\n",
        "\n",
        "  def highlight(self, orig, cor):\n",
        "      edits = self._get_edits(orig, cor)\n",
        "      orig_tokens = orig.split()\n",
        "\n",
        "      ignore_indexes = []\n",
        "\n",
        "      for edit in edits:\n",
        "          edit_type = edit[0]\n",
        "          edit_str_start = edit[1]\n",
        "          edit_spos = edit[2]\n",
        "          edit_epos = edit[3]\n",
        "          edit_str_end = edit[4]\n",
        "\n",
        "          # if no_of_tokens(edit_str_start) > 1 ==> excluding the first token, mark all other tokens for deletion\n",
        "          for i in range(edit_spos+1, edit_epos):\n",
        "            ignore_indexes.append(i)\n",
        "\n",
        "          if edit_str_start == \"\":\n",
        "              if edit_spos - 1 >= 0:\n",
        "                  new_edit_str = orig_tokens[edit_spos - 1]\n",
        "                  edit_spos -= 1\n",
        "              else:\n",
        "                  new_edit_str = orig_tokens[edit_spos + 1]\n",
        "                  edit_spos += 1\n",
        "              if edit_type == \"PUNCT\":\n",
        "                st = \"<a type='\" + edit_type + \"' edit='\" + \\\n",
        "                    edit_str_end + \"'>\" + new_edit_str + \"</a>\"\n",
        "              else:\n",
        "                st = \"<a type='\" + edit_type + \"' edit='\" + new_edit_str + \\\n",
        "                    \" \" + edit_str_end + \"'>\" + new_edit_str + \"</a>\"\n",
        "              orig_tokens[edit_spos] = st\n",
        "          elif edit_str_end == \"\":\n",
        "            st = \"<d type='\" + edit_type + \"' edit=''>\" + edit_str_start + \"</d>\"\n",
        "            orig_tokens[edit_spos] = st\n",
        "          else:\n",
        "            st = \"<c type='\" + edit_type + \"' edit='\" + \\\n",
        "                edit_str_end + \"'>\" + edit_str_start + \"</c>\"\n",
        "            orig_tokens[edit_spos] = st\n",
        "\n",
        "      for i in sorted(ignore_indexes, reverse=True):\n",
        "        del(orig_tokens[i])\n",
        "\n",
        "      return(\" \".join(orig_tokens))\n",
        "\n",
        "  def detect(self, input_sentence):\n",
        "        # TO BE IMPLEMENTED\n",
        "        pass\n",
        "\n",
        "  def _get_edits(self, orig, cor):\n",
        "        orig = self.annotator.parse(orig)\n",
        "        cor = self.annotator.parse(cor)\n",
        "        alignment = self.annotator.align(orig, cor)\n",
        "        edits = self.annotator.merge(alignment)\n",
        "\n",
        "        if len(edits) == 0:\n",
        "            return []\n",
        "\n",
        "        edit_annotations = []\n",
        "        for e in edits:\n",
        "            e = self.annotator.classify(e)\n",
        "            edit_annotations.append((e.type[2:], e.o_str, e.o_start, e.o_end,  e.c_str, e.c_start, e.c_end))\n",
        "\n",
        "        if len(edit_annotations) > 0:\n",
        "            return edit_annotations\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "  def get_edits(self, orig, cor):\n",
        "      return self._get_edits(orig, cor)"
      ],
      "metadata": {
        "id": "gbnsQBuo1kEU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gramformer import Gramformer\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1212)\n",
        "\n",
        "\n",
        "gf = Gramformer(models = 1, use_gpu=False) # 1=corrector, 2=detector\n",
        "\n",
        "#Applying and running on the some examples sentences\n",
        "influent_sentences = [\n",
        "    \"Matt like fish\",\n",
        "    \"the collection of letters was original used by the ancient Romans\",\n",
        "    \"We enjoys horror movies\",\n",
        "    \"Anna and Mike is going skiing\",\n",
        "    \"I walk to the store and I bought milk\",\n",
        "    \"We all eat the fish and then made dessert\",\n",
        "    \"I will eat fish for dinner and drank milk\",\n",
        "    \"what be the reason for everyone leave the company\",\n",
        "]\n",
        "\n",
        "for influent_sentence in influent_sentences:\n",
        "    corrected_sentences = gf.correct(influent_sentence, max_candidates=1)\n",
        "    print(\"[Input] \", influent_sentence)\n",
        "    for corrected_sentence in corrected_sentences:\n",
        "      print(\"[Correction] \",corrected_sentence)\n",
        "    print(\"-\" *100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFj9L3va2t_d",
        "outputId": "5b7a2e95-baf7-4796-b0c0-ef7c1f986430"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gramformer] Grammar error correct/highlight model loaded..\n",
            "[Input]  Matt like fish\n",
            "[Correction]  Matt likes fish.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  the collection of letters was original used by the ancient Romans\n",
            "[Correction]  the collection of letters was originally used by the ancient Romans\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  We enjoys horror movies\n",
            "[Correction]  We enjoy horror movies.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Anna and Mike is going skiing\n",
            "[Correction]  Anna and Mike are going skiing.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  I walk to the store and I bought milk\n",
            "[Correction]  I walked to the store and I bought milk.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  We all eat the fish and then made dessert\n",
            "[Correction]  We all ate the fish and then made dessert.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  I will eat fish for dinner and drank milk\n",
            "[Correction]  I ate fish for dinner and drank milk.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  what be the reason for everyone leave the company\n",
            "[Correction]  what is the reason for everyone leaving the company?\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Texting on the built customized dataset**"
      ],
      "metadata": {
        "id": "V6TzCBS_3NSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Responses_Dataset.tsv\", delimiter='\\t')\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.Responses.values\n",
        "right_sentences = df.Corrected_Responses.values #set of the correct sentences without errors\n",
        "\n",
        "len(sentences)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5LiEQBP2uLC",
        "outputId": "b0df6b19-357e-4578-84cf-12fd9f61cb61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#correcting the responses of ML interview questions\n",
        "correct_sentences  =  []  #set of generated correct sentences by our model\n",
        "for influent_sentence in sentences:\n",
        "    corrected_sentences = gf.correct(influent_sentence, max_candidates=1)\n",
        "    print(\"[Input] \", influent_sentence)\n",
        "    for corrected_sentence in corrected_sentences:\n",
        "      print(\"[Correction] \",corrected_sentence)\n",
        "      correct_sentences.append(corrected_sentence)\n",
        "    print(\"-\" *100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT4e0wky4Fpj",
        "outputId": "9f4da43e-2ae4-4f16-a568-60fc4b0eb9b0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Input]  One of the most common use is in market research and customer segmentation which is then utilized to target a particular market group to expand the businesses and profitable outcomes. \n",
            "[Correction]  One of the most common uses is in market research and customer segmentation which is then utilized to target a particular market group to expand the businesses and profitable outcomes.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine.\n",
            "[Correction]  The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The main principle behind this method is that if we will increase the number of clusters the error value will decrease.\n",
            "[Correction]  The main principle behind this method is that if we increase the number of clusters the error value will decrease.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Feature engineering refer to developing some new features by use existing features.\n",
            "[Correction]  Feature engineering refers to developing some new features by using existing features.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  A hypothesis is a term that is generally used in the Supervised machine learning domain. \n",
            "[Correction]  A hypothesis is a term that is generally used in the Supervised machine learning domain.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The Inertia or Sum of Squared Errors (SSE) and Silhouette score is an common metrics for measuring it the effectiveness of the clusters.\n",
            "[Correction]  The Inertia or Sum of Squared Errors (SSE) and Silhouette score are common metrics for measuring the effectiveness of the clusters.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Smaller values of learning rate help the training process to converge more slow and gradually toward  global optimum instead  fluctuating around it.\n",
            "[Correction]  Smaller values of learning rate help the training process to converge more slowly and gradually toward global optimum instead of fluctuating around it.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. \n",
            "[Correction]  Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values.\n",
            "[Correction]  The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If we use linear regression for the classification task the error function graph will not be convex.\n",
            "[Correction]  If we use linear regression for the classification task, the error function graph will not be convex.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values.\n",
            "[Correction]  To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Precision is simply the ratio behind the true positives and all the positive examples predict by model.\n",
            "[Correction]  Precision is simply the ratio behind the true positives and all the positive examples predicted by model.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  In the upsampling method, we increase the numbers of samples in the minority class by randomly selecting some points from the minority class or adding they to the dataset.\n",
            "[Correction]  In the upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class or adding them to the dataset.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage.\n",
            "[Correction]  If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Bias refers to the difference between the actual values and the predicted values by the model.\n",
            "[Correction]  Bias refers to the difference between the actual values and the predicted values by the model.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Variance refer to the changes in accuracy of the model’s prediction on which the model have not been trained.\n",
            "[Correction]  Variance refers to the changes in accuracy of the model’s prediction on which the model has not been trained.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If the bias are to low but the variance is too high then that case is known as overfitting.\n",
            "[Correction]  If the bias is too low but the variance is too high then that case is known as overfitting.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Principal Component Analysis is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly.\n",
            "[Correction]  Principal component analysis is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  One-shot learning is a concept in machine learning where the model is been trained to recognize the patterns in datasets from a single example instead of train on large datasets. \n",
            "[Correction]  One-shot learning is a concept in machine learning where the model is trained to recognize the patterns in datasets from a single example instead of train on large datasets.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. \n",
            "[Correction]  Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Euclidean Distance (ED) has calculated by the square root of the sum of squared differences among the coordinates of two points along each dimension.\n",
            "[Correction]  Euclidean distance (ED) is calculated by the square root of the sum of squared differences among the coordinates of two points along each dimension.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  As the name suggests, Covariance provides us with a measure of the extent to which two variables differ from each other.\n",
            "[Correction]  As the name suggests, covariance provides us with a measure of the extent to which two variables differ from each other.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Covariance can takes on any value while correlation is always between -1 and 1.\n",
            "[Correction]  Covariance can take on any value while correlation is always between -1 and 1.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented.\n",
            "[Correction]  Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones, the difference is in the way they are implemented.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data.\n",
            "[Correction]  If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  confusion matrix summarized the performance of a classification model. \n",
            "[Correction]  confusion matrix summarizes the performance of a classification model.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.\n",
            "[Correction]  From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The name violin plot has been derived from the shape of the graph which matches the violin and this graph is an extension to the Kernel Density Plot along the properties of the boxplot.\n",
            "[Correction]  The name violin plot has been derived from the shape of the graph which matches the violin and this graph is an extension to the Kernel Density Plot along the properties of the boxplot.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  In the gradient descent algorithm train our model on the whole dataset at once, but in Stochastic Gradient Descent, the model is being trained by using a mini-batch of training data at once.\n",
            "[Correction]  In the gradient descent algorithm, we train our model on the whole dataset at once, but in Stochastic Gradient Descent, the model is being trained by using a mini-batch of training data at once.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected.\n",
            "[Correction]  The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater then the median which is greater than the mode.\n",
            "[Correction]  In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater then the median which is greater than the mode.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Decision trees and random forests are both relatively robust to outliers. \n",
            "[Correction]  Decision trees and random forests are both relatively robust to outliers.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  RBF is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center. \n",
            "[Correction]  RBF is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  When two features is highly correlated, they may provide similar information to the model, which may causes overfitting.\n",
            "[Correction]  When two features are highly correlated, they may provide similar information to the model, which may cause overfitting.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(correct_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB50z3HYP-lw",
        "outputId": "a5f54a60-3d92-40a7-a2a6-3c2332e6f6bb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#highlighting the corrections in the input sentence for suggestions\n",
        "for influent_sentence in sentences:\n",
        "    corrected_sentences = gf.correct(influent_sentence, max_candidates=1)\n",
        "    print(\"[Input] \", influent_sentence)\n",
        "    for corrected_sentence in corrected_sentences:\n",
        "      print(\"[Edits] \", gf.highlight(influent_sentence, corrected_sentence))\n",
        "    print(\"-\" *100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwiRyRcx6MSZ",
        "outputId": "2b28ad2a-db74-432b-e5e8-cb7d119d1783"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Input]  One of the most common use is in market research and customer segmentation which is then utilized to target a particular market group to expand the businesses and profitable outcomes. \n",
            "[Edits]  One of the most common <c type='NOUN:NUM' edit='uses'>use</c> is in market research and customer segmentation which is then utilized to target a particular market group to expand the businesses and profitable outcomes.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine.\n",
            "[Edits]  The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The main principle behind this method is that if we will increase the number of clusters the error value will decrease.\n",
            "[Edits]  The main principle behind this method is that if we <d type='VERB:TENSE' edit=''>will</d> increase the number of clusters the error value will decrease.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Feature engineering refers to developing some new features by use of existing features.\n",
            "[Edits]  Feature engineering refers to developing some new features by use of existing features.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  A hypothesis is a term that is generally used in the Supervised machine learning domain. \n",
            "[Edits]  A hypothesis is a term that is generally used in the Supervised machine learning domain.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The Inertia or Sum of Squared Errors (SSE) and Silhouette score is an common metrics for measuring it the effectiveness of the clusters.\n",
            "[Edits]  The Inertia or Sum of Squared Errors (SSE) and Silhouette score <c type='VERB:SVA' edit='are'>is</c> <d type='DET' edit=''>an</d> common metrics for measuring <d type='PRON' edit=''>it</d> the effectiveness of the clusters.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Smaller values of learning rate help the training process to converge more slow and gradually toward  global optimum instead  fluctuating around it.\n",
            "[Edits]  Smaller values of learning rate help the training process to converge more <c type='MORPH' edit='slowly'>slow</c> and gradually toward global optimum <a type='PREP' edit='instead of'>instead</a> fluctuating around it.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. \n",
            "[Edits]  Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values.\n",
            "[Edits]  The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If we use linear regression for the classification task the error function graph will not be convex.\n",
            "[Edits]  If we use linear regression for the classification <c type='NOUN' edit='task,'>task</c> the error function graph will not be convex.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values.\n",
            "[Edits]  To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Precision is simply the ratio behind the true positives and all the positive examples predict by model.\n",
            "[Edits]  Precision is simply the ratio behind the true positives and all the positive examples <c type='VERB:FORM' edit='predicted'>predict</c> by model.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  In the upsampling method, we increase the numbers of samples in the minority class by randomly selecting some points from the minority class or adding they to the dataset.\n",
            "[Edits]  In the upsampling method, we increase the <c type='NOUN:NUM' edit='number'>numbers</c> of samples in the minority class by randomly selecting some points from the minority class or adding <c type='PRON' edit='them'>they</c> to the dataset.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage.\n",
            "[Edits]  If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Bias refers to the difference between the actual values and the predicted values by the model.\n",
            "[Edits]  Bias refers to the difference between the actual values and the predicted values by the model.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Variance refer to the changes in accuracy of the model’s prediction on which the model have not been trained.\n",
            "[Edits]  Variance <c type='VERB:SVA' edit='refers'>refer</c> to the changes in accuracy of the model’s prediction on which the model <c type='VERB:SVA' edit='has'>have</c> not been trained.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If the bias are to low but the variance is too high then that case is known as overfitting.\n",
            "[Edits]  If the bias <c type='VERB:SVA' edit='is'>are</c> <c type='SPELL' edit='too'>to</c> low but the variance is too high then that case is known as overfitting.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Principal Component Analysis is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly.\n",
            "[Edits]  Principal <c type='ORTH' edit='component analysis'>Component Analysis</c> is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  One-shot learning is a concept in machine learning where the model is been trained to recognize the patterns in datasets from a single example instead of train on large datasets. \n",
            "[Edits]  One-shot learning is a concept in machine learning where the model is <d type='VERB:TENSE' edit=''>been</d> trained to recognize the patterns in datasets from a single example instead of train on large datasets.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. \n",
            "[Edits]  Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Euclidean Distance (ED) has calculated by the square root of the sum of squared differences among the coordinates of two points along each dimension.\n",
            "[Edits]  Euclidean <c type='ORTH' edit='distance'>Distance</c> (ED) <c type='VERB:TENSE' edit='is'>has</c> calculated by the square root of the sum of squared differences among the coordinates of two points along each dimension.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  As the name suggests, Covariance provides us with a measure of the extent to which two variables differ from each other.\n",
            "[Edits]  As the name suggests, <c type='ORTH' edit='covariance'>Covariance</c> provides us with a measure of the extent to which two variables differ from each other.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Covariance can takes on any value while correlation is always between -1 and 1.\n",
            "[Edits]  Covariance can <c type='VERB:FORM' edit='take'>takes</c> on any value while correlation is always between -1 and 1.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented.\n",
            "[Edits]  <d type='OTHER' edit=''>One</d> Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric <c type='NOUN' edit='ones,'>ones</c> the difference is in the way they are implemented.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data.\n",
            "[Edits]  If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  confusion matrix summarized the performance of a classification model. \n",
            "[Edits]  confusion matrix <c type='VERB:TENSE' edit='summarizes'>summarized</c> the performance of a classification model.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.\n",
            "[Edits]  From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The name violin plot has been derived from the shape of the graph which matches the violin and this graph is an extension to the Kernel Density Plot along the properties of the boxplot.\n",
            "[Edits]  The name violin plot has been derived from the shape of the graph which matches the violin and this graph is an extension to the Kernel Density Plot along the properties of the boxplot.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  In the gradient descent algorithm train our model on the whole dataset at once, but in Stochastic Gradient Descent, the model is being trained by using a mini-batch of training data at once.\n",
            "[Edits]  In the gradient descent <a type='PRON' edit='<c type='OTHER' edit='algorithm,'>algorithm</c> we'><c type='OTHER' edit='algorithm,'>algorithm</c></a> train our model on the whole dataset at once, but in Stochastic Gradient Descent, the model is being trained by using a mini-batch of training data at once.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected.\n",
            "[Edits]  The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater then the median which is greater than the mode.\n",
            "[Edits]  In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater then the median which is greater than the mode.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Decision trees and random forests are both relatively robust to outliers. \n",
            "[Edits]  Decision trees and random forests are both relatively robust to outliers.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  RBF is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center. \n",
            "[Edits]  RBF is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et0ngB6V5lJp",
        "outputId": "cfca8041-ea44-4e9b-e5a4-a1babd637c38"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --requirements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GfO9W_k-X0P",
        "outputId": "a2653261-2e9e-406d-f229-d522d38c08b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip install [options] [-e] <vcs project url> ...\n",
            "  pip install [options] [-e] <local project path> ...\n",
            "  pip install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --requirements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLEU Score"
      ],
      "metadata": {
        "id": "euaSwftcBUeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required librarires for calculating the bleu score\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "#Applying and calculating on an example\n",
        "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
        "candidate = ['this', 'is', 'a', 'test']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP4RNY7_A71s",
        "outputId": "0ac17bf7-d59c-4947-b67e-e3c53d5668ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install pytorch_pretrained_bert the previous version of Pytorch-Transformers\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO_2XeVzB_wg",
        "outputId": "488faf7f-56cc-485b-b43e-eba207b3893d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m81.9/86.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.25.2)\n",
            "Collecting boto3 (from pytorch-pretrained-bert)\n",
            "  Downloading boto3-1.34.83-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting botocore<1.35.0,>=1.34.83 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading botocore-1.34.83-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.83->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.83->boto3->pytorch-pretrained-bert) (1.16.0)\n",
            "Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.34.83-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.34.83-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.34.83 botocore-1.34.83 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-pretrained-bert-0.6.2 s3transfer-0.10.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the sentences into tokens\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "tokenized_right_texts = [tokenizer.tokenize(sent) for sent in right_sentences]\n",
        "tokenized_correct_texts =  [tokenizer.tokenize(sent) for sent in correct_sentences]\n",
        "tokenized_texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zThZzHVRBZ67",
        "outputId": "28f8cf5c-2e10-433e-8e71-84cb522eda8e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'common',\n",
              " 'use',\n",
              " 'is',\n",
              " 'in',\n",
              " 'market',\n",
              " 'research',\n",
              " 'and',\n",
              " 'customer',\n",
              " 'segment',\n",
              " '##ation',\n",
              " 'which',\n",
              " 'is',\n",
              " 'then',\n",
              " 'utilized',\n",
              " 'to',\n",
              " 'target',\n",
              " 'a',\n",
              " 'particular',\n",
              " 'market',\n",
              " 'group',\n",
              " 'to',\n",
              " 'expand',\n",
              " 'the',\n",
              " 'businesses',\n",
              " 'and',\n",
              " 'profitable',\n",
              " 'outcomes',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_right_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4qGjDwwQImX",
        "outputId": "1155e59a-de19-400c-8106-da4280d7fb54"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = []"
      ],
      "metadata": {
        "id": "5D58L-DvRQ0Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a set for BLEU Scores\n",
        "BLEU_Score = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  reference.append(tokenized_right_texts[i])\n",
        "  candidate = tokenized_correct_texts[i]\n",
        "  score = sentence_bleu(reference, candidate)\n",
        "  BLEU_Score.append(score)\n",
        "  refernece = []\n",
        "\n",
        "BLEU_Score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqFHgGPlM-Bi",
        "outputId": "45c2b6e0-8f62-445d-950e-2555f5f7818a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8038026896544381,\n",
              " 1.0,\n",
              " 0.8747394842931362,\n",
              " 0.7048050905062194,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.9038693388414086,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.860789505997226,\n",
              " 1.0,\n",
              " 0.9240738952215708,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.9217324939947308,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.9257518071011758,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.9202722665493039,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.9255607445912747,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}