Responses	labels	Corrected_Responses		
One of the most common use is in market research and customer segmentation which is then utilized to target a particular market group to expand the businesses and profitable outcomes. 	1	One of the most common use is in market research and customer segmentation which is then utilised to target a particular market group to expand the businesses and profitable outcomes. 		
"The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine."	1	"The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine."		
The main principle behind this method is that if we will increase the number of clusters the error value will decrease.	1	The main principle behind this method is that if we will increase the number of clusters the error value will decrease.		
Feature engineering refer to developing some new features by use existing features.	0	Feature engineering refers to developing some new features by use of existing features.		
A hypothesis is a term that is generally used in the Supervised machine learning domain. 	1	A hypothesis is a term that is generally used in the Supervised machine learning domain. 		
The Inertia or Sum of Squared Errors (SSE) and Silhouette score is an common metrics for measuring it the effectiveness of the clusters.	0	The Inertia or Sum of Squared Errors (SSE) and Silhouette score are common metrics for measuring the effectiveness of the clusters.		
Smaller values of learning rate help the training process to converge more slow and gradually toward  global optimum instead  fluctuating around it.	0	Smaller values of learning rate help the training process to converge more slowly and gradually toward the global optimum instead of fluctuating around it.		
Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. 	1	Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. 		
"The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values."	1	"The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values."		
If we use linear regression for the classification task the error function graph will not be convex.	1	If we use linear regression for the classification task the error function graph will not be convex.		
To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values.	1	To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values.		
Precision is simply the ratio behind the true positives and all the positive examples predict by model.	0	Precision is simply the ratio behind the true positives and all the positive examples predicted by the model.		
"In the upsampling method, we increase the numbers of samples in the minority class by randomly selecting some points from the minority class or adding they to the dataset."	0	"In the upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class or adding them to the dataset."		
If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage.	1	If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage.		
Bias refers to the difference between the actual values and the predicted values by the model.	1	Bias refers to the difference between the actual values and the predicted values by the model.		
Variance refer to the changes in accuracy of the model’s prediction on which the model have not been trained.	0	Variance refers to the changes in the accuracy of the model’s prediction on which the model has not been trained.		
If the bias are to low but the variance is too high then that case is known as overfitting.	0	If the bias is too low but the variance is too high then that case is known as overfitting.		
Principal Component Analysis is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly.	1	Principal Component Analysis is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly.		
One-shot learning is a concept in machine learning where the model is been trained to recognize the patterns in datasets from a single example instead of train on large datasets. 	0	One-shot learning is a concept in machine learning where the model is trained to recognize the patterns in datasets from a single example instead of being trained on large datasets. 		
Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. 	1	Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. 		
Euclidean Distance (ED) has calculated by the square root of the sum of squared differences among the coordinates of two points along each dimension.	0	Euclidean Distance (ED) is calculated by the square root of the sum of squared differences among the coordinates of two points along each dimension.		
"As the name suggests, Covariance provides us with a measure of the extent to which two variables differ from each other."	1	"As the name suggests, Covariance provides us with a measure of the extent to which two variables differ from each other."		
Covariance can takes on any value while correlation is always between -1 and 1.	0	Covariance can take on any value while correlation is always between -1 and 1.		
One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented.	1	One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented.		
If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data.	1	If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data.		
confusion matrix summarized the performance of a classification model. 	0	Confusion matrix summarizes the performance of a classification model. 		
"From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc."	1	"From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc."		
The name violin plot has been derived from the shape of the graph which matches the violin and this graph is an extension to the Kernel Density Plot along the properties of the boxplot.	1	The name violin plot has been derived from the shape of the graph which matches the violin and this graph is an extension to the Kernel Density Plot along the properties of the boxplot.		
"In the gradient descent algorithm train our model on the whole dataset at once, but in Stochastic Gradient Descent, the model is being trained by using a mini-batch of training data at once."	0	"In the gradient descent algorithm, we train our model on the whole dataset at once, but in Stochastic Gradient Descent, the model is being trained by using a mini-batch of training data at once."		
The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected.	1	The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected.		
In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater then the median which is greater than the mode.	0	In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater than the median which is greater than the mode.		
Decision trees and random forests are both relatively robust to outliers. 	1	Decision trees and random forests are both relatively robust to outliers. 		
RBF is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center. 	1	RBF is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center. 		
"When two features is highly correlated, they may provide similar information to the model, which may causes overfitting."	0	"When two features are highly correlated, they may provide similar information to the model, which may cause overfitting."		